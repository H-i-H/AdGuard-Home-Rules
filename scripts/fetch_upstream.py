import requests
import os
from datetime import datetime
import time

# è§„åˆ™æºé…ç½®
SOURCES = {
    'ad': [
        'https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/Filters/AWAvenue-Ads-Rule.txt',
        'https://raw.githubusercontent.com/damengzhu/abpmerge/main/abpmerge.txt',  # ç›´æ¥ä½¿ç”¨åŸå§‹URL
    ],
    'privacy': [
        'https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_11_Mobile/filter.txt',
        'https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_3_Social_media/filter.txt',
    ],
    'malware': [
        'https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts',
        'https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_2_Browsing_security/filter.txt',
    ],
    'adult': [
        'https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_16_Adult/filter.txt',
    ]
}

def fetch_with_retry(url, max_retries=3, timeout=30):
    """å¸¦é‡è¯•çš„è¯·æ±‚å‡½æ•°"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            if response.status_code == 200:
                return response.text
            else:
                print(f"      âš ï¸  Status {response.status_code}, retry {attempt + 1}/{max_retries}")
        except Exception as e:
            print(f"      âŒ Error: {str(e)[:50]}, retry {attempt + 1}/{max_retries}")

        if attempt < max_retries - 1:
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

    return None

def fetch_and_save():
    os.makedirs('sources', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    success_count = 0

    for category, urls in SOURCES.items():
        print(f"\nğŸ“¥ Fetching {category} rules...")
        all_rules = []

        for i, url in enumerate(urls):
            print(f"  â†’ Source {i+1}: {url[:60]}...")
            content = fetch_with_retry(url)

            if content:
                lines = content.split('\n')
                all_rules.extend(lines)
                print(f"    âœ… Fetched {len(lines)} lines")
                success_count += 1
            else:
                print(f"    âŒ Failed to fetch: {url[:60]}...")

        # åªæœ‰åœ¨è·å–åˆ°æ•°æ®æ—¶æ‰ä¿å­˜
        if all_rules:
            filename = f'sources/{category}_{timestamp}.txt'
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f'! Category: {category}\n')
                f.write(f'! Fetched: {datetime.now()}\n')
                f.write(f'! Total sources: {len(urls)}\n')
                f.write('\n'.join(all_rules))

            print(f"  ğŸ’¾ Saved to {filename} ({len(all_rules)} total lines)")
        else:
            print(f"  âš ï¸  No rules fetched for {category}, skipping save")

    # æ£€æŸ¥æ•´ä½“æˆåŠŸç‡
    total_sources = sum(len(urls) for urls in SOURCES.values())
    if success_count == 0:
        print("\nâŒ Failed to fetch any rules!")
        return False
    elif success_count < total_sources:
        print(f"\nâš ï¸  Partial success: {success_count}/{total_sources} sources fetched")
        return True
    else:
        print(f"\nâœ… All upstream rules fetched successfully!")
        return True

if __name__ == '__main__':
    success = fetch_and_save()
    exit(0 if success else 1)
