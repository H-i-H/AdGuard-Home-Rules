import requests
import os
import time
from pathlib import Path
from urllib.parse import urlparse
from typing import Tuple, Optional

# ä¸Šæ¸¸è§„åˆ™æºé…ç½®
SOURCES = {
    'ads': [
        'https://adguardteam.github.io/AdGuardSDNSFilter/Filters/filter.txt',
        'https://easylist-downloads.adblockplus.org/easylist.txt',
        'https://easylist-downloads.adblockplus.org/easylistchina.txt'
    ],
    'malware': [
        'https://malware-filter.pages.dev/urlhaus-filter-online.txt',
        'https://malware-filter.pages.dev/phishing-filter.txt'
    ],
    'adult': [
        'https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/porn/hosts'
    ]
}

# è¯·æ±‚é…ç½®
REQUEST_TIMEOUT = 30
RETRY_DELAY = 2
MAX_RETRIES = 3
MIN_FILE_SIZE = 50  # æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

def get_filename_from_url(url: str) -> str:
    """ä»URLç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å"""
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '')
    path_parts = [p for p in parsed.path.split('/') if p]

    if path_parts:
        name = path_parts[-1].split('.')[0]  # å»æ‰æ‰©å±•å
        return f"{domain}_{name}.txt"
    return f"{domain}.txt"

def download_with_retry(url: str, filepath: Path, max_retries: int = MAX_RETRIES) -> Tuple[bool, str]:
    """å¸¦é‡è¯•æœºåˆ¶çš„ä¸‹è½½"""
    for attempt in range(max_retries):
        try:
            print(f"    ğŸ“¥ Attempt {attempt + 1}/{max_retries}: {url}")

            headers = {'User-Agent': USER_AGENT}
            response = requests.get(
                url,
                headers=headers,
                timeout=REQUEST_TIMEOUT,
                allow_redirects=True,
                verify=True
            )
            response.raise_for_status()

            # éªŒè¯å†…å®¹
            content = response.content
            if len(content) < MIN_FILE_SIZE:
                return False, f"File too small ({len(content)} bytes)"

            # æ£€æŸ¥æ˜¯å¦è¿”å›HTMLé”™è¯¯é¡µé¢
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' in content_type and 'filter' not in url:
                # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«HTMLæ ‡ç­¾
                if b'<html' in content[:100].lower() or b'<!doctype' in content[:100].lower():
                    return False, "Response appears to be HTML error page"

            # ä¿å­˜æ–‡ä»¶
            filepath.parent.mkdir(parents=True, exist_ok=True)
            with open(filepath, 'wb') as f:
                f.write(content)

            return True, f"Saved {len(content)} bytes"

        except requests.exceptions.Timeout:
            if attempt == max_retries - 1:
                return False, "Timeout after all retries"
            print(f"    âš ï¸  Timeout, retrying in {RETRY_DELAY}s...")
            time.sleep(RETRY_DELAY)

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                return False, "404 Not Found"
            elif e.response.status_code == 403:
                return False, "403 Forbidden (check User-Agent)"
            else:
                return False, f"HTTP {e.response.status_code}"

        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                return False, f"Network error: {e}"
            print(f"    âš ï¸  Error: {e}, retrying in {RETRY_DELAY}s...")
            time.sleep(RETRY_DELAY)

    return False, "Unknown error"

def fetch_all_sources():
    """è·å–æ‰€æœ‰ä¸Šæ¸¸è§„åˆ™"""
    print("ğŸ”„ Fetching upstream rules...")

    stats = {'success': 0, 'failed': 0, 'skipped': 0}

    for category, urls in SOURCES.items():
        print(f"\nğŸ“‚ Processing category: {category}")
        category_dir = Path('sources') / category
        category_dir.mkdir(parents=True, exist_ok=True)

        for i, url in enumerate(urls):
            filename = get_filename_from_url(url)
            filepath = category_dir / filename

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
            if filepath.exists():
                size = filepath.stat().st_size
                if size > MIN_FILE_SIZE:
                    print(f"    â­ï¸  Already exists: {filename} ({size} bytes)")
                    stats['skipped'] += 1
                    continue
                else:
                    # åˆ é™¤æ— æ•ˆçš„æ—§æ–‡ä»¶
                    filepath.unlink()

            success, message = download_with_retry(url, filepath)

            if success:
                print(f"    âœ… {message} -> {filename}")
                stats['success'] += 1
            else:
                print(f"    âŒ {message}")
                stats['failed'] += 1

            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(RETRY_DELAY)

    print(f"\nğŸ“Š Summary: {stats['success']} succeeded, {stats['failed']} failed, {stats['skipped']} skipped")
    return stats['failed'] == 0

def validate_downloaded_files():
    """éªŒè¯ä¸‹è½½çš„æ–‡ä»¶"""
    print("\nğŸ” Validating downloaded files...")

    issues = []
    for category in SOURCES.keys():
        category_dir = Path('sources') / category
        if not category_dir.exists():
            continue

        for filepath in category_dir.glob('*.txt'):
            size = filepath.stat().st_size
            if size == 0:
                issues.append(f"Empty file: {filepath}")
            elif size < MIN_FILE_SIZE:
                issues.append(f"Small file ({size} bytes): {filepath}")

            # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½
            try:
                content = filepath.read_text(encoding='utf-8', errors='ignore')
                if not content.strip():
                    issues.append(f"File contains only whitespace: {filepath}")
            except:
                issues.append(f"Cannot read file: {filepath}")

    if issues:
        print("  âš ï¸  Validation issues found:")
        for issue in issues:
            print(f"    - {issue}")
        return False

    print("  âœ… All files validated")
    return True

if __name__ == '__main__':
    success = fetch_all_sources()
    if success:
        validate_downloaded_files()
    else:
        print("\nâŒ Some downloads failed. Please check the logs.")
        exit(1)
