
import requests
import os
import time
from urllib.parse import urlparse

# ä¸Šæ¸¸è§„åˆ™æºé…ç½®
SOURCES = {
    'ads': [
        'https://adguardteam.github.io/AdGuardSDNSFilter/Filters/filter.txt',
        'https://easylist-downloads.adblockplus.org/easylist.txt',
        'https://easylist-downloads.adblockplus.org/easylistchina.txt'
    ],
    'malware': [
        'https://malware-filter.pages.dev/urlhaus-filter-online.txt',
        'https://malware-filter.pages.dev/phishing-filter.txt'
    ],
    'adult': [
        'https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/porn/hosts'
    ]
}

def download_file(url, filename):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
    try:
        print(f"  ğŸ“¥ Downloading: {url}")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # åˆ›å»ºç›®å½•
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"  âœ… Saved to: {filename}")
        return True
    except Exception as e:
        print(f"  âŒ Failed to download {url}: {e}")
        return False

def fetch_all_sources():
    """è·å–æ‰€æœ‰ä¸Šæ¸¸è§„åˆ™"""
    print("ğŸ”„ Fetching upstream rules...")
    
    for category, urls in SOURCES.items():
        print(f"\nğŸ“‚ Processing category: {category}")
        category_dir = os.path.join('sources', category)
        os.makedirs(category_dir, exist_ok=True)
        
        for i, url in enumerate(urls):
            filename = os.path.join(category_dir, f"{i+1}.txt")
            if not download_file(url, filename):
                print(f"  âš ï¸  Continuing with other sources...")
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            
    print("\nâœ… All upstream sources fetched!")
    return True

if __name__ == '__main__':
    fetch_all_sources()
